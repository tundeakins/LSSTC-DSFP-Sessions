{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Problem 2 - Practical HBM</h1>\n",
    "<h2 align=\"center\">One- and Two-Component Gaussian Mixture Hierarchcial Bayesian Modeling with PyJAGS</h2>\n",
    "<h3 align=\"center\">Simulating the eccentricity distribution of hot Jupiter exoplanets from the Kepler Mission</h3> \n",
    "<h4 align=\"center\">LSSTC DSFP Session 4, September 21st, 2017</h4>\n",
    "<h5 align=\"center\">Author: Megan I. Shabram, PhD, \n",
    "NASA Postdoctoral Program Fellow,  mshabram@gmail.com</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the code below to explore a two-component Gaussian mixture model\n",
    "2. Using the simulated data code cells below, evaluate the one-component truncated Gaussian generative model simualted data with the two-component Gaussian mixture HBM. What do you notice about the posteriors for the mixture fractions? \n",
    "3. Repeat this exersize but now evaluating simulated data from a one-component generative model with a two-component HBM. \n",
    "4. Notebook 3: Using the JAGS model code block for a break-point HBM, set up and evaluate the break-point HBM on the real Kepler eclipsing binary data set provided (some code is provided for analysis).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyjags\n",
    "import pickle\n",
    "\n",
    "from __future__ import division, print_function\n",
    "from pandas.tools.plotting import *\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"font.size\"] = 20\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#%qtconsole\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"left\"> 1. Generate simulated data set: one- and two-component truncated Gaussian mixture generative models:</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function to draw from truncated normal, this function will be used for both the \n",
    "## one- and two-componenet cases in this workbook.  \n",
    "\n",
    "def rnorm_bound( Ndata, mu, prec, lower_bound = 0.0, upper_bound = float('Inf')):\n",
    "    x = np.zeros(Ndata)\n",
    "    #print(x)\n",
    "    for i in range(0, Ndata): \n",
    "            #print(i)\n",
    "            while True:\n",
    "                x[i] = np.random.normal(mu,prec,1)\n",
    "                if( (x[i]>lower_bound) and (x[i]<upper_bound) ): \n",
    "                    break\n",
    "    return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the output to make sure the function is outputting as expected. \n",
    "#print(np.random.normal(0,0.0001,10))\n",
    "print(rnorm_bound(10, 0.0, 0.001, lower_bound=-1,upper_bound=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-component Gaussian Mixture Simulated Data for both projected eccentricity terms\n",
    "## Below we designate the population values of our generative model. These are the \n",
    "## truths that we should recover if our hiararchical Bayesian model is properly specified \n",
    "## and diagnostics have indicated that the simulation has \"not not converged\". \"You can't \n",
    "## prove convergence, at best you can fail to prove a failure to converge\".\n",
    "\n",
    "## In this simulated data set, their are 25 planetary systems (with one planet each)\n",
    "Ndata = 25 \n",
    "## Here we asign the dispersion of the simulated population to be 0.3, this is \n",
    "## the truth we wish to recovern \n",
    "sigmae = 0.3 \n",
    "## We approximate the uncertainty for each measurement as normally distributed about a \n",
    "## reporte measurement point estimate.  \n",
    "## For the eccentricity distribution for Hot Jupiters, the physical models used to derive \n",
    "## these produce larger uncertainty in k by a factor of 2. \n",
    "sigmahobs = 0.04\n",
    "sigmakobs = 0.08\n",
    "\n",
    "h = np.repeat(0.,Ndata)\n",
    "k = np.repeat(0.,Ndata)\n",
    "hhat = np.repeat(0.,Ndata)\n",
    "khat = np.repeat(0.,Ndata)\n",
    "hhat_sigma  = np.repeat(sigmahobs,Ndata)\n",
    "khat_sigma  = np.repeat(sigmakobs,Ndata)\n",
    "\n",
    "#print(khat_sigma)\n",
    "\n",
    "\n",
    "for i in range(0,Ndata):\n",
    "    h[i] = rnorm_bound(1,0,sigmae,lower_bound=-1,upper_bound=1)\n",
    "    lb = -np.sqrt(1-h[i]**2)\n",
    "    ub = np.sqrt(1-h[i]**2)\n",
    "    k[i] = rnorm_bound(1,0,sigmae,lower_bound=lb,upper_bound=ub) \n",
    "    hhat[i] = rnorm_bound(1,h[i],sigmahobs,lower_bound=-1,upper_bound=1)\n",
    "    khat[i] = rnorm_bound(1,k[i],sigmakobs,lower_bound=lb,upper_bound=ub)\n",
    "\n",
    "## Vizualize the true data values, and the simulated measurements:     \n",
    "print(h, hhat, k, khat)\n",
    "plt.hist(h)\n",
    "plt.hist(hhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## JAGS user manual: \n",
    "## http://www.uvm.edu/~bbeckage/Teaching/DataAnalysis/Manuals/manual.jags.pdf\n",
    "\n",
    "## JAGS model code\n",
    "\n",
    "## model code is the one-component truncated gaussian mixture HBM code from previous \n",
    "## notebook\n",
    "\n",
    "\n",
    "code1 = '''\n",
    "\n",
    "model {\n",
    "        \n",
    "    #Population parameters\n",
    "    e_sigma ~ dunif(0.0, 1.0)\n",
    "    e_phi <- 1/(e_sigma*e_sigma)\n",
    "        \n",
    "    for (n in 1:Ndata){\n",
    "    \n",
    "        #True planet properties\n",
    "        h[n] ~ dnorm(0, e_phi) T(-1,1) #Can try multivariate truncated normal in future\n",
    "        k[n] ~ dnorm(0, e_phi) T(-sqrt(1-h[n]*h[n]),sqrt(1-h[n]*h[n]))\n",
    "            \n",
    "        #Observed planet properties\n",
    "        hhat[n] ~ dnorm(h[n], 1.0/(hhat_sigma[n]*hhat_sigma[n])) T(-1,1)\n",
    "        khat[n] ~ dnorm(k[n], 1.0/(khat_sigma[n]*khat_sigma[n])) T(-sqrt(1-hhat[n]*hhat[n]),sqrt(1-hhat[n]*hhat[n]))\n",
    "    }\n",
    "        \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load additional JAGS module\n",
    "pyjags.load_module('glm')\n",
    "pyjags.load_module('dic')\n",
    "\n",
    "\n",
    "## See blog post for origination of the adapted analysis tools used here and below:\n",
    "## https://martynplummer.wordpress.com/2016/01/11/pyjags/\n",
    "\n",
    "num_chains = 4\n",
    "iterations = 10000\n",
    "\n",
    "\n",
    "## data list include only variables in the model\n",
    "model = pyjags.Model(code1, data=dict( Ndata=Ndata, hhat=hhat, khat=khat, \n",
    "                                     hhat_sigma=hhat_sigma, khat_sigma=khat_sigma), \n",
    "                     chains=num_chains, adapt=1000)\n",
    "\n",
    "## Code to speed up compute time. This feature might not be \n",
    "## well tested in pyjags at this time. \n",
    "## threads=4, chains_per_thread=1 \n",
    "\n",
    "## 500 warmup / burn-in iterations, not used for inference.\n",
    "model.sample(500, vars=[])\n",
    "\n",
    "## Run model for desired steps, monitoring hyperparameter variables, and latent variables\n",
    "## for hierarchical Bayesian model.\n",
    "## Returns a dictionary with numpy array for each monitored variable.\n",
    "## Shapes of returned arrays are (... shape of variable ..., iterations, chains).\n",
    "## samples = model.sample(#iterations per chain here, vars=['e_sigma', 'h', 'k'])\n",
    "samples = model.sample(iterations, vars=['e_sigma', 'h', 'k'])\n",
    "\n",
    "## Code to save, open and use pickled dictionary of samples:\n",
    "## -- Pickle the data --\n",
    "#with open('ecc_1_test.pkl', 'wb') as handle:\n",
    "#   pickle.dump(samples, handle)\n",
    "## -- Retrieve pickled data --\n",
    "#with open('ecc_1_test.pkl', 'rb') as handle:\n",
    "#   retrieved_results = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalize code for both h and k below. \n",
    "\n",
    "#print(samples)\n",
    "#print(samples.items())\n",
    "\n",
    "## Print and check the shape of the resultant samples dictionary:\n",
    "print(samples['e_sigma'].shape)\n",
    "print(samples['e_sigma'].squeeze(0).shape)\n",
    "print(samples['h'].shape)\n",
    "print(samples['k'][0,:,:].shape)\n",
    "print('-----')\n",
    "\n",
    "\n",
    "## Update the samples dictionary so that it includes keys for the latent variables\n",
    "## Also, we will use LaTeX formatting to help make legible plots ahead.  \n",
    "samples_Nm1 = {}\n",
    "\n",
    "## adjust the thin varible to only look at every 10th population element by setting it to 10\n",
    "thin = 1\n",
    "## Need to enter the number of hyperparameter variables here:\n",
    "numHyperParams = 1\n",
    "## Specify the dimension we want for our plot below, for legibility.  \n",
    "dim = (Ndata/thin)*2 + numHyperParams\n",
    "print(dim)\n",
    "\n",
    "for i in np.arange(0,Ndata,thin):\n",
    "    #hval = 'h'+str(i+1)\n",
    "    #kval = 'k'+str(i+1)\n",
    "    #print(hval)\n",
    "    #print(kval)\n",
    "    #samples_Nm1({hval: samples['h'][i,:,:]})\n",
    "    samples_Nm1.update({'$h_{'+str(i+1)+'}$': samples['h'][i,:,:], '$k_{'+str(i+1)+'}$': samples['k'][i,:,:]})\n",
    "#print(samples_2['h11'].shape)\n",
    "\n",
    "## Add the hyperparameter marginal posterior back in:\n",
    "samples_Nm1.update({'$e_{\\sigma}$': samples['e_sigma'].squeeze(0)})\n",
    "\n",
    "## Below, examine the updated and reformatted sample dictionary to include keys for \n",
    "## latent variables \n",
    "for j, i in samples_Nm1.items():\n",
    "    print(j)\n",
    "    print(i)\n",
    "samples_Nm1['$h_{5}$'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>  <p>Below is code to look at <b>summary statistics</b> of <b>the marginal posterior distributions</b>  (the probabilistic parameter estimates) \n",
    "for the hyperparameter and the latent variables \n",
    "(each population constituent), in this case <i>h</i> and <i>k</i> (a.k.a projected eccentricity here), of the exoplanet systems we are simulating). </p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## equal tailed 95% credible intervals, and posterior distribution means:\n",
    "def summary(samples, varname, p=95):\n",
    "    values = samples[varname][0]\n",
    "    ci = np.percentile(values, [100-p, p])\n",
    "    print('{:<6} mean = {:>5.1f}, {}% credible interval [{:>4.1f} {:>4.1f}]'.format(\n",
    "      varname, np.mean(values), p, *ci))\n",
    "\n",
    "for varname in samples_Nm1:\n",
    "    summary(samples_Nm1, varname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use pandas three dimensional Panel to represent the trace:\n",
    "\n",
    "trace = pd.Panel({k: v for k, v in samples_Nm1.items()})\n",
    "trace.axes[0].name = 'Variable'\n",
    "trace.axes[1].name = 'Iteration'\n",
    "trace.axes[2].name = 'Chain'\n",
    " \n",
    "## Point estimates:\n",
    "print(trace.to_frame().mean())\n",
    " \n",
    "## Bayesian equal-tailed 95% credible intervals:\n",
    "print(trace.to_frame().quantile([0.05, 0.95]))\n",
    "  ## ^ entering the values here could be a good question part\n",
    "    \n",
    "def plot(trace, var):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "    fig.suptitle(var, y=0.95, fontsize='xx-large')\n",
    " \n",
    "    ## Marginal posterior density estimate:\n",
    "    trace[var].plot.density(ax=axes[0])\n",
    "    axes[0].set_xlabel('Parameter value')\n",
    "    axes[0].locator_params(tight=True)\n",
    " \n",
    "    ## Autocorrelation for each chain:\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    for chain in trace[var].columns:\n",
    "        autocorrelation_plot(trace[var,:,chain], axes[1], label=chain)\n",
    " \n",
    "    ## Trace plot:\n",
    "    axes[2].set_ylabel('Parameter value')\n",
    "    trace[var].plot(ax=axes[2])\n",
    " \n",
    "    ## Save figure\n",
    "    filename = var.replace(\"\\\\\", \"\") \n",
    "    filename = filename.replace(\"$\", \"\") \n",
    "    filename = filename.replace(\"}\", \"\") \n",
    "    filename = filename.replace(\"{\", \"\") \n",
    "    plt.tight_layout(pad=3)\n",
    "    fig.savefig('Nm1_JAGS_h_and_k_'+'{}.png'.format(filename))\n",
    " \n",
    "# Display diagnostic plots\n",
    "for var in trace:\n",
    "    plot(trace, var)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter matrix plot:\n",
    "\n",
    "## Redefine the trace so that we only vizualize every 10th latent variable element in \n",
    "## the scatter_matrix plot below. Vizualizing all 50 is too cumbersome for the scatter\n",
    "## matrix. \n",
    "\n",
    "samples_Nm1_for_scatter_matrix = {}\n",
    "Nsamp=25\n",
    "## adjust the thin varible to only look at every 10th population element by setting it to 10\n",
    "thin = 10\n",
    "numHyperParams = 1\n",
    "dim = (Nsamp/thin)*2 + numHyperParams\n",
    "print(dim)\n",
    "\n",
    "for i in np.arange(0,Nsamp,thin):\n",
    "    samples_Nm1_for_scatter_matrix.update({'$h_{'+str(i+1)+'}$': samples['h'][i,:,:], '$k_{'+str(i+1)+'}$': samples['k'][i,:,:]})\n",
    "samples_Nm1_for_scatter_matrix.update({'$e_{\\sigma}$': samples['e_sigma'].squeeze(0)})\n",
    "\n",
    "for j, i in samples_Nm1_for_scatter_matrix.items():\n",
    "    print(j)\n",
    "#    print(i)\n",
    "\n",
    "trace_2 = pd.Panel({k: v for k, v in samples_Nm1_for_scatter_matrix.items()})\n",
    "\n",
    "sm = scatter_matrix(trace_2.to_frame(),  color=\"darkturquoise\", alpha=0.2, figsize=(dim*2, dim*2), diagonal='hist',hist_kwds={'bins':25,'histtype':'step', 'edgecolor':'r','linewidth':2})\n",
    "## y labels size\n",
    "[plt.setp(item.yaxis.get_label(), 'size', 20) for item in sm.ravel()]\n",
    "## x labels size \n",
    "[plt.setp(item.xaxis.get_label(), 'size', 20) for item in sm.ravel()]\n",
    "## Change label rotation\n",
    "## This is helpful for very long labels\n",
    "#[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n",
    "[s.xaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "## May need to offset label when rotating to prevent overlap of figure\n",
    "[s.get_yaxis().set_label_coords(-0.5,0.5) for s in sm.reshape(-1)]\n",
    "## Hide all ticks\n",
    "#[s.set_xticks(()) for s in sm.reshape(-1)]\n",
    "#[s.set_yticks(()) for s in sm.reshape(-1)]\n",
    "\n",
    "plt.savefig('scatter_matrix_Nm1_JAGS.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"left\"> 2. Generalize to a two-component Gaussian Mixture Population Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below we designate the population values of our two-component truncated Gaussian \n",
    "## generative model. These are the truths that we should recover if our hiararchical \n",
    "## Bayesian model is properly specified and diagnostics have indicated that the simulation \n",
    "## has \"not not converged\".\n",
    "Ndata = 50\n",
    "Nm = 2\n",
    "frac = [0.7,0.3]\n",
    "sigmae = [0.05,0.3]\n",
    "\n",
    "## After generating values from the population model, we now add realistic Gaussian noise \n",
    "## to create simulated measurements. \n",
    "sigmahobs = 0.04\n",
    "sigmakobs = 0.08\n",
    "\n",
    "h = np.repeat(0.,Ndata)\n",
    "k = np.repeat(0.,Ndata)\n",
    "hhat = np.repeat(0.,Ndata)\n",
    "khat = np.repeat(0.,Ndata)\n",
    "hhat_sigma  = np.repeat(sigmahobs,Ndata)\n",
    "khat_sigma  = np.repeat(sigmakobs,Ndata)\n",
    "\n",
    "#print(khat_sigma)\n",
    "\n",
    "for i in range(0,Ndata):\n",
    "    #print('i')\n",
    "    #print(i)\n",
    "    \n",
    "    c = np.random.choice(len(frac), 1, p=frac, replace=True)\n",
    "    #print(int(c))\n",
    "    h[i] = rnorm_bound(1,0,sigmae[int(c)],lower_bound=-1,upper_bound=1)\n",
    "    # Euler's formula: h^2 + k^2 = 1\n",
    "    lb = -np.sqrt(1-h[i]**2)\n",
    "    ub = np.sqrt(1-h[i]**2)\n",
    "    k[i] = rnorm_bound(1,0,sigmae[int(c)],lower_bound=lb,upper_bound=ub) \n",
    "    hhat[i] = rnorm_bound(1,h[i],sigmahobs,lower_bound=-1,upper_bound=1)\n",
    "    khat[i] = rnorm_bound(1,k[i],sigmakobs,lower_bound=lb,upper_bound=ub)\n",
    "\n",
    "    \n",
    "print(h, hhat, k, khat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# JAGS model code\n",
    "\n",
    "code = '''\n",
    "\n",
    "model {\n",
    "        \n",
    "    #Population parameters\n",
    "    for (j in 1:Nm) {\n",
    "        e_sigma[j] ~ dunif(0.0, 1.0)\n",
    "        e_phi[j] <- 1/(e_sigma[j]*e_sigma[j])\n",
    "        a[j] <- 1;\n",
    "    }\n",
    "\n",
    "    f ~ ddirch(a[])\n",
    "        \n",
    "    for (n in 1:Ndata){\n",
    "    \n",
    "        #True planet properties\n",
    "        c[n] ~ dcat(f[]) \n",
    "        h[n] ~ dnorm(0, e_phi[c[n]]) T(-1,1) #Can try multivariate truncated normal in future\n",
    "        k[n] ~ dnorm(0, e_phi[c[n]]) T(-sqrt(1-h[n]*h[n]),sqrt(1-h[n]*h[n]))\n",
    "            \n",
    "        #Observed planet properties\n",
    "        hhat[n] ~ dnorm(h[n], 1.0/(hhat_sigma[n]*hhat_sigma[n])) T(-1,1)\n",
    "        khat[n] ~ dnorm(k[n], 1.0/(khat_sigma[n]*khat_sigma[n])) T(-sqrt(1-hhat[n]*hhat[n]),sqrt(1-hhat[n]*hhat[n]))\n",
    "    }\n",
    "        \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional JAGS module\n",
    "pyjags.load_module('glm')\n",
    "pyjags.load_module('dic')\n",
    "\n",
    "#data list include only variables in the model\n",
    "model = pyjags.Model(code, data=dict(Nm=Nm, Ndata=Ndata, hhat=hhat, khat=khat, \n",
    "                                     hhat_sigma=hhat_sigma, khat_sigma=khat_sigma), \n",
    "                     chains=4, adapt=1000)\n",
    "# threads=4, chains_per_thread=1 \n",
    "# 500 warmup / burn-in iterations, not used for inference.\n",
    "model.sample(500, vars=[])\n",
    "\n",
    "## Run model for desired steps, monitoring hyperparameter variables.\n",
    "## Returns a dictionary with numpy array for each monitored variable.\n",
    "## Shapes of returned arrays are (... shape of variable ..., iterations, chains).\n",
    "## \n",
    "iters = 1000000\n",
    "samples2 = model.sample(iters, vars=['e_sigma', 'h', 'k', 'c', 'f'])\n",
    "\n",
    "# Pickle the data\n",
    "#with open('ecc_1_test.pkl', 'wb') as handle:\n",
    "#    pickle.dump(samples, handle)\n",
    "    \n",
    "# Retrieve pickled data\n",
    "# with open('ecc_1_test.pkl', 'rb') as handle:\n",
    "#      retrieved_results = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the high and low mixture components to accomidate the exchangeability of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_thin = 100\n",
    "start = int(iters-1000)\n",
    "esigma_low = np.where(samples2['e_sigma'][0,start::,:] <= samples2['e_sigma'][1,start::,:], samples2['e_sigma'][0,start::,:], samples2['e_sigma'][1,start::,:])\n",
    "esigma_hi = np.where(samples2['e_sigma'][0,start::,:] > samples2['e_sigma'][1,start::,:], samples2['e_sigma'][0,start::,:], samples2['e_sigma'][1,start::,:])\n",
    "f_low = np.where(samples2['e_sigma'][0,start::,:] <= samples2['e_sigma'][1,start::,:], samples2['f'][0,start::,:], samples2['f'][1,start::,:])\n",
    "f_hi = np.where(samples2['e_sigma'][0,start::,:] > samples2['e_sigma'][1,start::,:], samples2['f'][0,start::,:], samples2['f'][1,start::,:])\n",
    "print(np.min(f_hi))\n",
    "plt.hist(f_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iters = 1000000\n",
    "\n",
    "#print(samples)\n",
    "#print(samples.items())\n",
    "\n",
    "print(samples2['h'].shape)\n",
    "print(samples2['k'][0,start::,:].shape)\n",
    "print('-----')\n",
    "samples_Nm2 = {}\n",
    "Nsamp=50\n",
    "thin = 1\n",
    "numHyperParams = 4\n",
    "dim = (Nsamp/thin)*2 + numHyperParams\n",
    "print(dim)\n",
    "for i in np.arange(0,Nsamp,thin):\n",
    "    samples_Nm2.update({'$h_{'+str(i+1)+'}$': samples2['h'][i,start::,:], '$k_{'+str(i+1)+'}$': samples2['k'][i,start::,:]})\n",
    "samples_Nm2.update({'$e_{\\sigma_{low}}$': esigma_low, '$e_{\\sigma_{high}}$': esigma_hi })\n",
    "samples_Nm2.update({'$f_{low}$': f_low,'$f_{high}$': f_hi })\n",
    "\n",
    "for j, i in samples_Nm2.items():\n",
    "    print(j)\n",
    "    #print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## equal tailed 95% credible intervals, and posterior distribution means:\n",
    "def summary(samples, varname, p=95):\n",
    "    values = samples[varname][0]\n",
    "    ci = np.percentile(values, [100-p, p])\n",
    "    print('{:<6} mean = {:>5.1f}, {}% credible interval [{:>4.1f} {:>4.1f}]'.format(\n",
    "      varname, np.mean(values), p, *ci))\n",
    "\n",
    "for varname in samples_Nm2:\n",
    "    summary(samples_Nm2, varname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Use pandas three dimensional Panel to represent the trace:\n",
    "\n",
    "trace = pd.Panel({k: v for k, v in samples_Nm2.items()})\n",
    "trace.axes[0].name = 'Variable'\n",
    "trace.axes[1].name = 'Iteration'\n",
    "trace.axes[2].name = 'Chain'\n",
    " \n",
    "## Point estimates:\n",
    "print(trace.to_frame().mean())\n",
    " \n",
    "## Bayesian equal-tailed 95% credible intervals:\n",
    "print(trace.to_frame().quantile([0.05, 0.95]))\n",
    "  ## ^ entering the values here could be a good question part\n",
    "    \n",
    "def plot(trace, var):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "    fig.suptitle(var, y=0.95, fontsize='xx-large')\n",
    " \n",
    "    ## Marginal posterior density estimate:\n",
    "    trace[var].plot.density(ax=axes[0])\n",
    "    axes[0].set_xlabel('Parameter value')\n",
    "    axes[0].locator_params(tight=True)\n",
    " \n",
    "    ## Autocorrelation for each chain:\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    for chain in trace[var].columns:\n",
    "        autocorrelation_plot(trace[var,:,chain], axes[1], label=chain)\n",
    " \n",
    "    ## Trace plot:\n",
    "    axes[2].set_ylabel('Parameter value')\n",
    "    trace[var].plot(ax=axes[2])\n",
    " \n",
    "    ## Save figure\n",
    "    filename = var.replace(\"\\\\\", \"\") \n",
    "    filename = filename.replace(\"/\", \"\") \n",
    "    filename = filename.replace(\"$\", \"\") \n",
    "    filename = filename.replace(\"}\", \"\") \n",
    "    filename = filename.replace(\"{\", \"\") \n",
    "    plt.tight_layout(pad=3)\n",
    "    fig.savefig('Nm2_JAGS_'+'{}.png'.format(filename))\n",
    "\n",
    "    \n",
    "## Display diagnostic plots\n",
    "for var in trace:\n",
    "    plot(trace, var)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter matrix plot:\n",
    "\n",
    "## Redefine the trace so that we only vizualize every 10th latent variable element in \n",
    "## the scatter_matrix plot below. Vizualizing all 50 is too cumbersome for the scatter\n",
    "## matrix. \n",
    "\n",
    "samples_Nm2_for_scatter_matrix = {}\n",
    "\n",
    "## adjust the thin varible to only look at every 10th population element by setting it to 10\n",
    "thin = 10\n",
    "numHyperParams = 4\n",
    "dim = (Ndata/thin)*2 + numHyperParams\n",
    "print(dim)\n",
    "\n",
    "for i in np.arange(0,Ndata,thin):\n",
    "    samples_Nm2_for_scatter_matrix.update({'$h_{'+str(i+1)+'}$': samples2['h'][i,start::,:], '$k_{'+str(i+1)+'}$': samples2['k'][i,start::,:]})\n",
    "samples_Nm2_for_scatter_matrix.update({'$e_{\\sigma_{low}}$': esigma_low, '$e_{\\sigma_{high}}$': esigma_hi })\n",
    "samples_Nm2_for_scatter_matrix.update({'$f_{low}$': f_low,'$f_{high}$': f_hi })\n",
    "\n",
    "for j, i in samples_Nm2_for_scatter_matrix.items():\n",
    "    print(j)\n",
    "#    print(i)\n",
    "\n",
    "trace_3 = pd.Panel({k: v for k, v in samples_Nm2_for_scatter_matrix.items()})\n",
    "\n",
    "sm = scatter_matrix(trace_3.to_frame(),  color=\"darkturquoise\", alpha=0.2, figsize=(dim*2, dim*2), diagonal='hist',hist_kwds={'bins':25,'histtype':'step', 'edgecolor':'r','linewidth':2})\n",
    "## y labels size\n",
    "[plt.setp(item.yaxis.get_label(), 'size', 20) for item in sm.ravel()]\n",
    "## x labels size \n",
    "[plt.setp(item.xaxis.get_label(), 'size', 20) for item in sm.ravel()]\n",
    "## Change label rotation\n",
    "## This is helpful for very long labels\n",
    "#[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n",
    "[s.xaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "## May need to offset label when rotating to prevent overlap of figure\n",
    "[s.get_yaxis().set_label_coords(-0.5,0.5) for s in sm.reshape(-1)]\n",
    "## Hide all ticks\n",
    "#[s.set_xticks(()) for s in sm.reshape(-1)]\n",
    "#[s.set_yticks(()) for s in sm.reshape(-1)]\n",
    "\n",
    "plt.savefig('scatter_matrix_Nm2_JAGS.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
