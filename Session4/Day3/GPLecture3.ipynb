{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian process regression \n",
    "\n",
    "## Lecture 3: Bayesian Quadrature\n",
    "\n",
    "### Suzanne Aigrain, University of Oxford\n",
    "\n",
    "#### LSST DSFP Session 4, Seattle, Sept 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian quadrature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... is a model-based approach to numerical integration, where a GP prior is placed on the integrand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Setting up a GP model over the function to be integrated enables analytic estimation of the posterior mean and variance of the integrand (as well as the form of the integrand itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consider the integral\n",
    "\n",
    "$$\n",
    "Z = \\int f(x) \\, p(x) \\, \\mathrm{d}x\n",
    "$$\n",
    "\n",
    "where $p(x)$ is a probability density, and $f(x)$ is the function we wish to integrate. For example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- given a model with parameters $x$, $p(x)$ could be the posterior over the parameters, and $f(x)$ the predictions of the model, so that $Z$ is the posterior mean of model predictions, or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p(x)$ could be a prior and $f(x)=\\mathrm{p}(y \\, | \\, x)$ the likelihood, so that $Z$ is the marginal likelihood, or evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical Monte Carlo\n",
    "\n",
    "approximates $Z$ as\n",
    "\n",
    "$$\n",
    "Z \\approx \\frac{1}{T} \\sum_{t=1}^T f(x^{(t)})\n",
    "$$\n",
    "\n",
    "where $x^{(t)}$ are random (not necessarily independent) draws from $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Importance sampling\n",
    "\n",
    "Usually, sampling from true $p(x)$ directly is difficult, so draw the samples the samples $x^{(t)}$ from a more tractable importance sampling distribution $q(x)$ instead, then\n",
    "\n",
    "$$\n",
    "Z = \\int \\frac{f(x) \\, p(x)}{q(x)} \\, q(x) \\, \\mathrm{d}x \\approx \\frac{1}{T} \\sum_{t=1}^T \\frac{f(x^{(t)}) \\, p(x^{(t)})}{q(x^{(t)})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo is fundamentally unsound\n",
    "(O'Hagan 1989)\n",
    "\n",
    "### Problem 1: depends on irrelevant information\n",
    "\n",
    "Estimate of $Z$ from importance sampling depends on the values of $f(x^{(t)})\\,p(x^{(t)})$, but also on the en-\n",
    "tirely arbitrary choice of the sampling distribution $q(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If the same set of samples $\\{x^{(1)} , \\ldots , x^{(T)} \\}$, conveying exactly the same information about $Z$,  were obtained from two different sampling distributions, two different estimates of $Z$ would be obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo is fundamentally unsound\n",
    "(O'Hagan 1989)\n",
    "\n",
    "### Problem 2: ignores relevant information\n",
    "\n",
    "Classical Monte Carlo procedures ignore the values of the $x^{(t)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Imagine three samples $\\{ x^{(1)}, x^{(2)}, x^{(3)} \\}$, where it just happens by chance that $x^{(1)}=x^{(3)}$. Classical MC simply averages over the 3 sampels ,which is clearly inappropriate. It would be much better to use only $x^{(1)}$ and $x^{(2)}$, or only $x^{(1)}$ and $x^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Of course such a situation is unlikely in real life, but the values of the $\\{ x^{(t)}\\}$ clearly contain relevant information that shouldn't be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Integration as a Bayesian inference problem\n",
    "\n",
    "Treat $Z$ as a random variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since $Z$ is a function of $f(x)$, which is unknown until we evaluate it, proceed by putting a prior on $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Gaussian Process is a convenient way of putting a prior on a function. Also, the integral becomes analytic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is no reason to expect $f$ to be Gaussian distributed in general, so this prior is not particularly appropriate, but we don't know of a more general family of distributions over functions that would be as tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Monte Carlo \n",
    "(O'Hagan 1991, Rassmussen & Garhamani 2003)\n",
    "\n",
    "Start with zero-mean GP prior over $f$. Typically use squared exponential covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given a set of samples $\\mathcal{D} = \\{(x^{(i)},f(x^{(i)})\\,|\\,i=1,\\ldots,N\\}$, the posterior over $f$, $\\mathrm{p}(f\\,|\\,\\mathcal{D})$, is Gaussian (and is given by the standard GP predictive equations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As the integral $Z = \\int f(x) \\, p(x) \\, \\mathrm{d}x$ is just a linear projection (along the direction defined by $f$), the posterior over Z is also Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/BQ1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/BQ3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/BQ7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Expectation of $Z$\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mathbb{E}_{[f|\\mathcal{D}]}[Z] & = & \\int \\int f(x) \\, p(x) \\, \\mathrm{d}x \\, \\mathrm{p}(f\\,|\\,\\mathcal{D}) \\mathrm{d}f \\\\\n",
    "& = & \\int \\int f(x) \\, \\mathrm{p}(f\\,|\\,\\mathcal{D}) \\, \\mathrm{d}f \\, p(x) \\, \\mathrm{d}x \\\\\n",
    "& = & \\int \\overline{f}_{\\mathcal{D}} \\, p(x) \\, \\mathrm{d}x \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\overline{f}_{\\mathcal{D}}$ is the posterior mean, or expectation, of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variance of $Z$\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mathbb{V}_{[f|\\mathcal{D}]}[Z] & = & \\int \\left[ \\int f(x) \\, p(x) \\, \\mathrm{d}x - \\int \\overline{f}(x') \\, p(x') \\, \\mathrm{d}x' \\right] \\, \\mathrm{p}(f\\,|\\,\\mathcal{D}) \\, \\mathrm{d}f \\\\\n",
    "& = & \\int \\int \\int \\left[ f(x) - \\overline{f}(x) \\right] \\, \\left[ f(x') - \\overline{f}(x') \\right] \\mathrm{p}(f\\,|\\,\\mathcal{D}) \\, \\mathrm{d}f \\, p(x) \\, p(x') \\mathrm{d}x \\, \\mathrm{d}x' \\\\\n",
    "& = & \\int \\mathrm{Cov}_{\\mathcal{D}} (f(x), f(x')) \\, p(x) \\, p(x') \\mathrm{d}x \\, \\mathrm{d}x' \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{Cov}_{\\mathcal{D}}$ is the posterior covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the standard GP equations. The posterior mean is given by:\n",
    "\n",
    "$$\n",
    "\\overline{f}(x) = k(x, \\mathbf{x}) \\, K^{-1} \\, \\mathbf{f}\n",
    "$$\n",
    "\n",
    "and the posterior covariance by\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(f(x),f(x')) = k(x, x') - k(x,\\mathbf{x}) \\, K^{-1} \\, k(\\mathbf{x},x)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}$ and $\\mathbf{f}$ are the observed inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore, defining $\\mathbf{z}$ such that $z_t =  \\int k(x, x^{(t)}) \\,p(x) \\, \\mathrm{d}x$,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mathbb{E}_{[f|\\mathcal{D}]}[Z] & = & \\mathbf{z}^T \\, K^{-1} \\, \\mathbf{f}\\,~\\mathrm{and} \\\\\n",
    "\\mathbb{V}_{[f|\\mathcal{D}]}[Z] & = &  \\int \\int k(x,x') \\, p(x) \\, p(x') \\mathrm{d}x \\mathrm{d}x' - \\mathbf{z}^T K^{-1} \\mathbf{z}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some things to note:\n",
    "- The variance of $Z$ is a natural convergence diagnostic\n",
    "- It doesn't depend on $f$ at all! Can devise optimal sampling scheme in advance\n",
    "- This expression ignores uncertainty on GP hyper-parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general the integrals in $\\mathbb{E}_{[f|\\mathcal{D}]}[Z]$ and $\\mathbb{V}_{[f|\\mathcal{D}]}[Z]$ are not tractable, but if the prior $p(x)$ is conjugate with the kernel $k(x,x')$, they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance\n",
    "\n",
    "<img src=\"images/BMC_performance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Bayes-Hermite Quadrature \n",
    "(O-Hagan 1991)\n",
    "\n",
    "In particular, if we have a Gaussian prior $p(x) =  \\mathcal{N}(\\mathbf{b},B)$ with mean $\\mathbf{b}$ and variance $B$, and covariance function of the GP over $f$ can be written\n",
    "Gaussian kernels on the data points are $\\mathcal{N} (a_i = x^{(i)} , A = \\mathrm{diag}(w_1^2 , \\ldots, w_D^2 ))$, then the expectation evaluates to:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{[f|\\mathcal{D}]}[Z] = \\mathbf{z}^⊤ \\, K^{−1} \\, \\mathbf{f},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = w_0 |A^{−1} B + I|^{−1/2} \\exp \\left[ −0.5 (\\mathbf{a}−\\mathbf{b})^⊤ (A+B)^{−1} (\\mathbf{a}−\\mathbf{b}) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "and the variance to\n",
    "$$\n",
    "\\mathbb{V}_{[f|\\mathcal{D}]}[Z] = w_0 |2 A^{−1} B + I|^{−1/2} - z^T \\, K^{-1} \\, z\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other forms of prior\n",
    "\n",
    "Similarly, polynomials and mixtures of Gaussians for $p(x)$ lead to analytical expressions for the $\\mathbb{E}_{[f|\\mathcal{D}]}[Z]$ and $\\mathbb{V}_{[f|\\mathcal{D}]}[Z]$. \n",
    "\n",
    "To be able to use any priors, need to resort to importance reweighting trick (Kennedy 1998):\n",
    "\n",
    "$$\n",
    "Z = \\int \\frac{f(x) \\, p(x)}{q(x)} \\, q(x) \\, \\mathrm{d}x \n",
    "$$\n",
    "\n",
    "where the Gaussian process models $f(x) \\, p(x)/q(x)$ and $q(x)$ is Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To understand what is meant by \"the Gaussian kernels on the data points\", recall that the predictive mean for $f$ can be expressed as a linear combination of kernel functions centred on the observed inputs:\n",
    "$$\n",
    "\\overline{f}(x) = \\sum_{i=1}^N \\alpha_i k(x_i,x),\n",
    "$$\n",
    "where $\\alpha_i = (K + \\sigma^2 I)^{-1} \\, f_i$. \n",
    "\n",
    "In our case, we can evaluate $f(x)$ exactly, so $\\sigma=0$ and $\\alpha_i = K^{-1} \\, f_i$. \n",
    "\n",
    "If $k(x,x')= h^2 \\exp \\left[ -(x-x')/2l^2 \\right]$, then ... \n",
    "\n",
    "MORE WORK NEEDED\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GP prior on log likelihood\n",
    "Osborne et al. (2012)\n",
    "\n",
    "When using Bayesian quadrature to evaluate evidence, $f$ represents a likelihood. Applying the GP to $\\log f$ rather than $f$ ensures it is always positive, and helps match the typically large dynamic range.\n",
    "\n",
    "<img src=\"images/BQ_GPL.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This severely affects results if likelihood peak is sharp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GP prior on log likelihood\n",
    "Osborne et al. (2012)\n",
    "\n",
    "When using Bayesian quadrature to evaluate evidence, $f$ represents a likelihood. Applying the GP to $\\log f$ rather than $f$ ensures it is always positive, and helps match the typically large dynamic range.\n",
    "\n",
    "<img src=\"images/BQ_GPlogL.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Drawback: integration no longer analytic. Approximate using Taylor expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Marginalising over the GP hyper-parameters\n",
    "Osborne et al. (2012)\n",
    "\n",
    "This matters a lot for the accuracy of the variance estimate.\n",
    "\n",
    "<img src=\"images/BBQ2.png\">\n",
    "\n",
    "Drawback: requires multiple layers of inference. Computational cost per sample high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BBQ (\"doubly Bayesian quadrature\")\n",
    "Osborne et al. (2012)\n",
    "\n",
    "When marginalising over HPs, variance of $Z$ **does** depend on samples. Opens up the possibility of active sampling.\n",
    "\n",
    "<img src=\"images/BBQ3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further improvements: WSABI\n",
    "Gunter et al. (2014)\n",
    "\n",
    "Model $\\sqrt{\\mathcal{L}}$ insteald of $\\log \\mathcal{L}$. Goes some way towards matching dynamic range, while simplifying inference and making it more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Code available in python: \n",
    "- https://github.com/OxfordML/wsabi\n",
    "- https://github.com/mareksyldatk/WaSABI_Py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RV and astrometric searches for planets\n",
    "\n",
    "<img src=\"images/HD208587.png\">\n",
    "\n",
    "Highly multimodal posteriors, likelihood can be expensive (e.g. if modelling activity)\n",
    "\n",
    "Need to compare evidence for 0, 1, 2, ... planets. C.f. \"evidence challenge\" at EPRV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "a86bdae1-f5f8-41c8-8d03-33324866d981",
    "theme": {
     "72cad74a-f4c5-4868-a59f-a1b18bef32b2": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "72cad74a-f4c5-4868-a59f-a1b18bef32b2",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         0,
         43,
         54
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         232,
         213
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         38,
         139,
         210
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         147,
         161,
         161
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     },
     "a86bdae1-f5f8-41c8-8d03-33324866d981": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "a86bdae1-f5f8-41c8-8d03-33324866d981",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         247,
         251,
         252
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         51,
         51,
         51
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         59,
         117,
         158
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         51,
         51,
         51
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Quicksand",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Quicksand",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Quicksand",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Quicksand",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Quicksand"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Quicksand"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Quicksand"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Open Sans",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Open Sans",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Open Sans",
       "font-size": 5
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
